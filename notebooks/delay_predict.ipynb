{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0c6820-debd-48c9-91d6-2f6dd14ab0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as sps\n",
    "from hdfs3 import HDFileSystem\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78b3cbf1-4688-44e3-830c-e6f0c078a94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Can not find the shared library: libhdfs3.so\n",
      "See installation instructions at http://hdfs3.readthedocs.io/en/latest/install.html\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/site-packages/hdfs3/core.py\", line 88, in __init__\n",
      "    self.connect()\n",
      "  File \"/usr/lib/python2.7/site-packages/hdfs3/core.py\", line 106, in connect\n",
      "    get_lib()\n",
      "  File \"/usr/lib/python2.7/site-packages/hdfs3/core.py\", line 670, in get_lib\n",
      "    from .lib import _lib as l\n",
      "  File \"/usr/lib/python2.7/site-packages/hdfs3/lib.py\", line 24, in <module>\n",
      "    raise ImportError(\"Can not find the shared library: libhdfs3.so\\n\"\n",
      "ImportError: Can not find the shared library: libhdfs3.so\n",
      "See installation instructions at http://hdfs3.readthedocs.io/en/latest/install.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hdfs = HDFileSystem(user='ayu')\n",
    "\n",
    "def read_hdfs(path):\n",
    "    files = hdfs.glob(path)\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        with hdfs.open(file) as f:\n",
    "            dfs.append(pd.read_parquet(f))\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7991b50f-ab40-47e9-bff6-a443336fa2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "global name 'hdfs' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 2, in read_hdfs\n",
      "NameError: global name 'hdfs' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Delay averages\n",
    "default = read_hdfs('/user/%s/final/parquet/default/*.parquet' % username)\n",
    "t_type_line = read_hdfs('/user/%s/final/parquet/t_type_line/*.parquet' % username)\n",
    "t_type_line_hour = read_hdfs('/user/%s/final/parquet/t_type_line_hour/*.parquet' % username)\n",
    "t_type_line_hour_stop = read_hdfs('/user/%s/final/parquet/t_type_line_hour_stop/*.parquet' % username)\n",
    "\n",
    "# Dicts to speedup average delay lookup times\n",
    "t_type_line_dict = t_type_line.set_index(['transport_type', 'line_name']).to_dict()['avg_delay']\n",
    "t_type_line_hour_dict = t_type_line_hour.set_index(['transport_type', 'line_name', 'hour']).to_dict()['avg_delay']\n",
    "t_type_line_hour_stop_dict = t_type_line_hour_stop.set_index(['transport_type', 'line_name', 'hour', 'stop_id']).to_dict()['avg_delay']\n",
    "default_delay = default.values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8be283-9b8b-4b57-ad19-a116853a6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_delay_probability(max_delay, t_type, line_name, hour, stop_id):\n",
    "    \"\"\"\n",
    "    t_type: transport_type(PRODUCT_ID)\n",
    "    line_name: line_name(linien_text)\n",
    "    hour: hour for arrival_scheduled\n",
    "    stop_id: stop station id\n",
    "    \"\"\"\n",
    "    prob = 1.0\n",
    "    if (t_type, line_name, hour, stop_id) in t_type_line_hour_stop_dict:\n",
    "        delay_pred = t_type_line_hour_stop_dict[(t_type, line_name, hour, stop_id)]\n",
    "    elif (t_type, line_name, hour) in t_type_line_hour_dict:\n",
    "        delay_pred = t_type_line_hour_dict[(t_type, line_name, hour)]\n",
    "    elif (t_type, line_name) in t_type_line_dict:\n",
    "        delay_pred = t_type_line_hour_dict[(t_type, line_name)]\n",
    "    else:\n",
    "        delay_pred = default_delay\n",
    "                \n",
    "    if lambda_ != 0:\n",
    "        #prob *= exponential_cdf(delay_pred, max_delay * 60)\n",
    "        prob = sps.expon.cdf(max_delay, scale=1/delay_pred)\n",
    "        \n",
    "    return prob\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
